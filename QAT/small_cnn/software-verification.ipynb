{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Exported ONNX Model in FINN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import model into FINN with ModelWrapper\n",
    "\n",
    "The quantized model is initialised in a ModelWrapper to test how the model behaves with its new FINN structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "model_dir = \"\"\n",
    "ready_model_filename = model_dir + \"cnn_ready_qout.onnx\"\n",
    "model_for_sim = ModelWrapper(ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINN provides a number of functions to access information about the model. This can be used to verify information like the model inputs/outputs and the model shape are still correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: global_in\n",
      "Output tensor name: global_out\n",
      "Input tensor shape: [1, 2, 16, 16]\n",
      "Output tensor shape: [1, 4]\n",
      "Input tensor datatype: INT8\n",
      "Output tensor datatype: BIPOLAR\n",
      "List of node operator types in the graph: \n",
      "['MultiThreshold', 'Add', 'Mul', 'Conv', 'Mul', 'BatchNormalization', 'MultiThreshold', 'Mul', 'MaxPool', 'Flatten', 'MatMul', 'Mul', 'MultiThreshold']\n"
     ]
    }
   ],
   "source": [
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model_for_sim.graph.node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output tensor is (as of yet) marked as a float32 value, even though we know the output is binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network preparation: Tidy-up transformations\n",
    "\n",
    "Before running the verification, we need to prepare our FINN-ONNX model. In particular, all the intermediate tensors need to have statically defined shapes. To do this, we apply some graph transformations to the model like a kind of \"tidy-up\" to make it easier to process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "verif_model_filename = model_dir + \"cnn-verification.onnx\"\n",
    "model_for_sim.save(verif_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "\n",
    "The dataset is loaded as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_strings(lst):\n",
    "    filtered_list = [s for s in lst if not any(digit in s for digit in \"3456789\")]\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "folder = \"../fullPlutoImport\"\n",
    "files = os.listdir(folder)\n",
    "\n",
    "filtered_files = filter_strings(files)\n",
    "\n",
    "factor = 2\n",
    "noFiles = len(filtered_files)\n",
    "\n",
    "arr = np.ndarray((int(7800*noFiles/factor),128*factor*2), float)\n",
    "labels = np.ndarray((int(7800*noFiles/factor),4))\n",
    "\n",
    "seed = 0\n",
    "\n",
    "i = 0;\n",
    "for idx, npz in enumerate(filtered_files):\n",
    "    \n",
    "    a = np.load(os.path.join(folder, npz))\n",
    "    \n",
    "    start_idx = (idx*int(7800/factor)) if idx <20 else (idx)*int(7800/factor)-1\n",
    "    end_idx = (1+idx)*int(7800/factor) if idx <20 else (1+idx)*int(7800/factor)-1\n",
    "           \n",
    "    reshaped_arr = a[\"samples\"].reshape(int(7800/factor), 128*factor)\n",
    "    \n",
    "    float_array = np.ndarray((int(7800/factor), 128*factor*2), float)\n",
    "    for j in range(reshaped_arr.shape[0]):\n",
    "        float_array[j] = np.ravel((reshaped_arr[j].real, reshaped_arr[j].imag),'F')\n",
    "    arr[start_idx:end_idx] = float_array\n",
    "    labels[start_idx:end_idx] = np.tile(a[\"active_channels\"],  (int(7800/factor), 1))\n",
    "\n",
    "    i+=1\n",
    "    if i >= noFiles:\n",
    "        break\n",
    "    \n",
    "normalized_array = 255 * (arr + 2) / (4) - 128\n",
    "\n",
    "normalized_array = normalized_array.astype(np.int8)\n",
    "\n",
    "ver_arr = TensorDataset(torch.tensor(normalized_array, dtype=torch.float32), torch.tensor(labels, dtype=torch.int8))\n",
    "\n",
    "\n",
    "n_verification_inputs = 100\n",
    "input_tensor = ver_arr.tensors[0][:n_verification_inputs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuild CNN\n",
    "\n",
    "The model is remade in Brevitas using the same weights as before using its state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bits = 4\n",
    "a_bits = 4\n",
    "w_bits = 4\n",
    "filters_conv = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import IntBias\n",
    "from brevitas.inject.enum import ScalingImplType\n",
    "from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class InputQuantizer(Int8ActPerTensorFloatMinMaxInit):\n",
    "    bit_width = input_bits\n",
    "    min_val = -2.0\n",
    "    max_val = 2.0\n",
    "    scaling_impl_type = ScalingImplType.CONST # Fix the quantization range to [min_val, max_val]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # Input quantization layer\n",
    "    qnn.QuantHardTanh(act_quant=InputQuantizer),\n",
    "\n",
    "    qnn.QuantConv2d(2, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm2d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    nn.MaxPool2d(2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "\n",
    "    qnn.QuantLinear(filters_conv*8*8, 4, weight_bit_width=w_bits, bias=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_334019/3741863630.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained_state_dict = torch.load(\"state_dict_self-trained.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment the following line if you previously chose to train the network yourself\n",
    "trained_state_dict = torch.load(\"state_dict_self-trained.pth\")\n",
    "\n",
    "model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: quantize the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "class BipolarForExport(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(BipolarForExport, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.qnt_output = QuantIdentity(\n",
    "            quant_type='binary', \n",
    "            scaling_impl_type='const',\n",
    "            bit_width=1, min_val=-1.0, max_val=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out_original = self.pretrained(x)\n",
    "        out_final = self.qnt_output(out_original)   # output as {-1,1}     \n",
    "        return out_final\n",
    "\n",
    "\n",
    "\n",
    "model = BipolarForExport(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare FINN & Brevitas execution <a id=\"compare_brevitas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINN provides the finn.core.onnx_exec function to simulate what happens in FINN with the given model. By executing on it using this function it can be verified that the model will act in the same manor as the Brevitas model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def inference_with_finn_onnx(current_inp):\n",
    "    finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "    finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "    finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "    # convert input to numpy for FINN\n",
    "    current_inp = current_inp.detach().numpy()\n",
    "    # reshape to expected input (add 1 for batch dimension)\n",
    "    current_inp = current_inp.reshape(finnonnx_model_in_shape)\n",
    "    # create the input dictionary\n",
    "    input_dict = {finnonnx_in_tensor_name : current_inp} \n",
    "    # run with FINN's execute_onnx\n",
    "    output_dict = oxe.execute_onnx(model_for_sim, input_dict)\n",
    "    #get the output tensor\n",
    "    finn_output = output_dict[finnonnx_out_tensor_name] \n",
    "    return finn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get outputs from the brevitas model, simply run as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_brevitas(current_inp):\n",
    "    model.eval() \n",
    "    brevitas_output = model(current_inp)\n",
    "\n",
    "    return brevitas_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the inference helper functions are called for each input and the outputs compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FINN execution:   0%|          | 0/100 [00:00<?, ?it/s]/home/dspedia/.local/lib/python3.10/site-packages/torch/_tensor.py:1488: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1928.)\n",
      "  return super().rename(names)\n",
      "/home/dspedia/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:549: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:311.)\n",
      "  return F.conv2d(\n",
      "/home/dspedia/.local/lib/python3.10/site-packages/qonnx/util/basic.py:299: UserWarning: The values of tensor MultiThreshold_0_out0 can't be represented with the set datatype annotation (INT4), they will be rounded to match the datatype annotation.\n",
      "  warnings.warn(\n",
      "ok 100 nok 0: 100%|██████████| 100/100 [00:06<00:00, 16.65it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "verify_range = trange(n_verification_inputs, desc=\"FINN execution\", position=0, leave=True)\n",
    "model.eval()\n",
    "\n",
    "ok = 0\n",
    "nok = 0\n",
    "\n",
    "for i in verify_range:\n",
    "    # run in Brevitas with PyTorch tensor\n",
    "    current_inp = input_tensor[i].reshape((1, 2,16,16))\n",
    "    brevitas_output = inference_with_brevitas(current_inp).detach().numpy()\n",
    "    finn_output = inference_with_finn_onnx(current_inp)\n",
    "    # compare the outputs\n",
    "    ok += 1 if (finn_output==brevitas_output).all() else 0\n",
    "    nok += 1 if (finn_output != brevitas_output).any() else 0\n",
    "    verify_range.set_description(\"ok %d nok %d\" % (ok, nok))\n",
    "    verify_range.refresh()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert ok == n_verification_inputs\n",
    "    print(\"Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical\")\n",
    "except AssertionError:\n",
    "    assert False, \"Verification failed. Brevitas and FINN-ONNX execution outputs are NOT identical\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
