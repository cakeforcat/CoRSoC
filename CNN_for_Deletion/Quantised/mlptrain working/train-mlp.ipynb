{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Quantized MLP on UNSW-NB15 with Brevitas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Live FINN tutorial:** We recommend clicking **Cell -> Run All** when you start reading this notebook for \"latency hiding\".</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to create, train and export a quantized Multi Layer Perceptron (MLP) with quantized weights and activations with [Brevitas](https://github.com/Xilinx/brevitas).\n",
    "Specifically, the task at hand will be to label network packets as normal or suspicious (e.g. originating from an attacker, virus, malware or otherwise) by training on a quantized variant of the UNSW-NB15 dataset. \n",
    "\n",
    "**You won't need a GPU to train the neural net.** This MLP will be small enough to train on a modern x86 CPU, so no GPU is required to follow this tutorial  Alternatively, we provide pre-trained parameters for the MLP if you want to skip the training entirely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick introduction to the task and the dataset\n",
    "\n",
    "*The task:* The goal of [*network intrusion detection*](https://ieeexplore.ieee.org/abstract/document/283931) is to identify, preferably in real time, unauthorized use, misuse, and abuse of computer systems by both system insiders and external penetrators. This may be achieved by a mix of techniques, and machine-learning (ML) based techniques are increasing in popularity. \n",
    "\n",
    "*The dataset:* Several datasets are available for use in ML-based methods for intrusion detection.\n",
    "The **UNSW-NB15** is one such dataset created by the Australian Centre for Cyber Security (ACCS) to provide a comprehensive network based data set which can reflect modern network traffic scenarios. You can find more details about the dataset on [its homepage](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/).\n",
    "\n",
    "*Performance considerations:* FPGAs are commonly used for implementing high-performance packet processing systems that still provide a degree of programmability. To avoid introducing bottlenecks on the network, the DNN implementation must be capable of detecting malicious ones at line rate, which can be millions of packets per second, and is expected to increase further as next-generation networking solutions provide increased\n",
    "throughput. This is a good reason to consider FPGA acceleration for this particular use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "\n",
    "* [Load the UNSW_NB15 Dataset](#load_dataset) \n",
    "* [Define a PyTorch Device](#define_pytorch_device)\n",
    "* [Define the Quantized MLP Model](#define_quantized_mlp)\n",
    "* [Define Train and Test  Methods](#train_test)\n",
    "    * [(Option 1) Train the Model from Scratch](#train_scratch)\n",
    "    * [(Option 2) Load Pre-Trained Parameters](#load_pretrained)\n",
    "* [Network Surgery Before Export](#network_surgery)\n",
    "* [Export to QONNX and Conversion to FINN-ONNX](#export_qonnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import onnx\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_dir = \"\"#os.environ['FINN_ROOT'] + \"/notebooks/end2end_example/cybersecurity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is important -- always import onnx before torch**. This is a workaround for a [known bug](https://github.com/onnx/onnx/issues/2394)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the UNSW_NB15 Dataset <a id='load_dataset'></a>\n",
    "\n",
    "### Dataset Quantization <a id='dataset_qnt'></a>\n",
    "\n",
    "The goal of this notebook is to train a Quantized Neural Network (QNN) to be later deployed as an FPGA accelerator generated by the FINN compiler. Although we can choose a variety of different precisions for the input, [Murovic and Trost](https://ev.fe.uni-lj.si/1-2-2019/Murovic.pdf) have previously shown we can actually binarize the inputs and still get good (90%+) accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a binarized representation for the dataset by following the procedure defined by Murovic and Trost, which we repeat briefly here:\n",
    "\n",
    "* Original features have different formats ranging from integers, floating numbers to strings.\n",
    "* Integers, which for example represent a packet lifetime, are binarized with as many bits as to include the maximum value. \n",
    "* Another case is with features formatted as strings (protocols), which are binarized by simply counting the number of all different strings for each feature and coding them in the appropriate number of bits.\n",
    "* Floating-point numbers are reformatted into fixed-point representation.\n",
    "* In the end, each sample is transformed into a 593-bit wide binary vector. \n",
    "* All vectors are labeled as bad (0) or normal (1)\n",
    "\n",
    "Following Murovic and Trost's open-source implementation provided as a Matlab script [here](https://github.com/TadejMurovic/BNN_Deployment/blob/master/cybersecurity_dataset_unswb15.m), we've created a [Python version](dataloader_quantized.py).\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** Downloading the original dataset and quantizing it can take some time, so we provide a download link to the pre-quantized version for your convenience. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the binarized numpy arrays from the .npz archive and wrap them as a PyTorch `TensorDataset` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_array(arr):\n",
    "\n",
    "  # find the minimum and maximum values in the array.\n",
    "  min_val = np.min(arr)\n",
    "  max_val = np.max(arr)\n",
    "\n",
    "  normalized_arr = 256 * (arr - min_val) / (max_val - min_val) - 128\n",
    "\n",
    "  return normalized_arr\n",
    "\n",
    "\n",
    "def filter_strings(lst):\n",
    "    filtered_list = [s for s in lst if not any(digit in s for digit in \"23456789\")]\n",
    "    return filtered_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array min: -2.0, max: 2.0\n",
      "Normalized array min: -128.0, max: 128.0\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset\n",
    "\n",
    "# def get_preqnt_dataset(data_dir: str, train: bool):\n",
    "#     unsw_nb15_data = np.load(data_dir + \"/unsw_nb15_binarized.npz\")\n",
    "#     if train:\n",
    "#         partition = \"train\"\n",
    "#     else:\n",
    "#         partition = \"test\"\n",
    "#     part_data = unsw_nb15_data[partition].astype(np.float32)\n",
    "#     part_data = torch.from_numpy(part_data)\n",
    "#     part_data_in = part_data[:, :-1]\n",
    "#     part_data_out = part_data[:, -1]\n",
    "#     return TensorDataset(part_data_in, part_data_out)\n",
    "\n",
    "# train_quantized_dataset = get_preqnt_dataset(\".\", True)\n",
    "# test_quantized_dataset = get_preqnt_dataset(\".\", False)\n",
    "\n",
    "# print(\"Samples in each set: train = %d, test = %s\" % (len(train_quantized_dataset), len(test_quantized_dataset))) \n",
    "# print(\"Shape of one input sample: \" +  str(train_quantized_dataset[0][0].shape))\n",
    "\n",
    "import numpy as np\n",
    "import os as os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "#from torchsummary import summary\n",
    "import brevitas.nn as qnn\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder = \"../fullPlutoImport\"\n",
    "files = os.listdir(folder)\n",
    "\n",
    "filtered_files = filter_strings(files)\n",
    "\n",
    "factor = 1\n",
    "noFiles = len(filtered_files)\n",
    "\n",
    "arr = np.ndarray((int(7800*noFiles/factor),2,128*factor), float)\n",
    "labels = np.ndarray((int(7800*noFiles/factor),4))\n",
    "\n",
    "seed = 42\n",
    "\n",
    "i = 0;\n",
    "for idx, npz in enumerate(filtered_files):\n",
    "    \n",
    "    a = np.load(os.path.join(folder, npz))\n",
    "    \n",
    "    start_idx = (idx*int(7800/factor)) if idx <20 else (idx)*int(7800/factor)-1\n",
    "    end_idx = (1+idx)*int(7800/factor) if idx <20 else (1+idx)*int(7800/factor)-1\n",
    "    \n",
    "    # print(f\"start index: {start_idx}, end index {end_idx}, activate channels: {a['active_channels']}\")\n",
    "       \n",
    "    reshaped_arr = a[\"samples\"].reshape(int(7800/factor), 128*factor)\n",
    "    \n",
    "    float_array = np.stack((reshaped_arr.real, reshaped_arr.imag), axis=1) \n",
    "\n",
    "    #float_array = float_array.reshape(int(7800/factor), 2*128*factor)\n",
    "\n",
    "    arr[start_idx:end_idx] = float_array\n",
    "    labels[start_idx:end_idx] = np.tile(a[\"active_channels\"],  (int(7800/factor), 1))\n",
    "\n",
    "    i+=1\n",
    "    if i >= noFiles:\n",
    "        break\n",
    "    \n",
    "normalized_array = normalize_array(arr)\n",
    "labels = labels[:-1]\n",
    "\n",
    "\n",
    "print(f\"Original array min: {np.min(arr)}, max: {np.max(arr)}\")\n",
    "print(f\"Normalized array min: {np.min(normalized_array)}, max: {np.max(normalized_array)}\")\n",
    "\n",
    "arr = normalized_array[:-1]\n",
    "\n",
    "# first split into train+val and test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(arr, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "# then split train+val into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=seed)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up DataLoader\n",
    "\n",
    "Following either option, we now have access to the quantized dataset. We will wrap the dataset in a PyTorch `DataLoader` for easier access in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape for 1 batch: torch.Size([256, 2, 128])\n",
      "Label shape for 1 batch: torch.Size([256, 4])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for x,y in train_loader:\n",
    "    print(\"Input shape for 1 batch: \" + str(x.shape))\n",
    "    print(\"Label shape for 1 batch: \" + str(y.shape))\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-12.7365, -50.9459,  25.4730,  ...,   0.0000,  63.6824,  12.7365],\n",
      "         [ 50.9459,  25.4730,  25.4730,  ..., -12.7365, -12.7365,   0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,  57.2433,  ...,   0.0000, -28.6217,  28.6217],\n",
      "         [-28.6217,   0.0000,  28.6217,  ...,  28.6217, -28.6217,   0.0000]],\n",
      "\n",
      "        [[ 17.7504,   0.0000,   0.0000,  ..., -35.5008, -35.5008,  17.7504],\n",
      "         [ 35.5008,  53.2512,  71.0016,  ...,  35.5008,  53.2512,   0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0.0000,  30.1699,  30.1699,  ...,  30.1699,   0.0000,   0.0000],\n",
      "         [ 30.1699,  30.1699,  30.1699,  ...,   0.0000,   0.0000,  30.1699]],\n",
      "\n",
      "        [[ 63.1292,  21.0431,   0.0000,  ..., -21.0431, -21.0431,   0.0000],\n",
      "         [-42.0861,  42.0861,  21.0431,  ...,  42.0861,  21.0431, -21.0431]],\n",
      "\n",
      "        [[  0.0000, -15.0849, -15.0849,  ...,   0.0000, -30.1699,  30.1699],\n",
      "         [-60.3398,   0.0000,  60.3398,  ..., -75.4247,  45.2548,  30.1699]]])\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset.tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a PyTorch Device <a id='define_pytorch_device'></a> \n",
    "\n",
    "GPUs can significantly speed-up training of deep neural networks. We check for availability of a GPU and if so define it as target device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Quantized MLP Model <a id='define_quantized_mlp'></a>\n",
    "\n",
    "We'll now define an MLP model that will be trained to perform inference with quantized weights and activations.\n",
    "For this, we'll use the quantization-aware training (QAT) capabilities offered by [Brevitas](https://github.com/Xilinx/brevitas).\n",
    "\n",
    "Our MLP will have four fully-connected (FC) layers in total: three hidden layers with 64 neurons, and a final output layer with a single output, all using 2-bit weights. We'll use 2-bit quantized ReLU activation functions, and apply batch normalization between each FC layer and its activation.\n",
    "\n",
    "In case you'd like to experiment with different quantization settings or topology parameters, we'll define all these topology settings as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2048      \n",
    "hidden1 = 64      \n",
    "hidden2 = 64\n",
    "hidden3 = 64\n",
    "weight_bit_width = 2\n",
    "act_bit_width = 2\n",
    "num_classes = 4    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our MLP using the layer primitives provided by Brevitas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantHardTanh(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): Identity()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ConstScaling(\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): Identity()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_init_module): Identity()\n",
       "            (value): StatelessBuffer()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): QuantLinear(\n",
       "    in_features=2048, out_features=64, bias=True\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Dropout(p=0.5, inplace=False)\n",
       "  (4): QuantReLU(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): ReLU()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "            (stats_input_view_shape_impl): OverTensorView()\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsPercentile()\n",
       "            )\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "            (restrict_scaling): _RestrictValue(\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (clamp_scaling): _ClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "            )\n",
       "            (restrict_inplace_preprocess): Identity()\n",
       "            (restrict_preprocess): Identity()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (5): QuantLinear(\n",
       "    in_features=64, out_features=64, bias=True\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout(p=0.5, inplace=False)\n",
       "  (8): QuantReLU(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): ReLU()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "            (stats_input_view_shape_impl): OverTensorView()\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsPercentile()\n",
       "            )\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "            (restrict_scaling): _RestrictValue(\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (clamp_scaling): _ClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "            )\n",
       "            (restrict_inplace_preprocess): Identity()\n",
       "            (restrict_preprocess): Identity()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (9): QuantLinear(\n",
       "    in_features=64, out_features=64, bias=True\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Dropout(p=0.5, inplace=False)\n",
       "  (12): QuantReLU(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): ReLU()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "            (stats_input_view_shape_impl): OverTensorView()\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsPercentile()\n",
       "            )\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "            (restrict_scaling): _RestrictValue(\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (clamp_scaling): _ClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "            )\n",
       "            (restrict_inplace_preprocess): Identity()\n",
       "            (restrict_preprocess): Identity()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (13): QuantLinear(\n",
       "    in_features=64, out_features=4, bias=True\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantLinear, QuantReLU, QuantIdentity\n",
    "import brevitas.quant.fixed_point\n",
    "import torch.nn as nn\n",
    "from brevitas.inject.enum import ScalingImplType\n",
    "from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class InputQuantizer(Int8ActPerTensorFloatMinMaxInit):\n",
    "    bit_width = act_bit_width\n",
    "    min_val = -2.0\n",
    "    max_val = 2.0\n",
    "    scaling_impl_type = ScalingImplType.CONST # Fix the quantization range to [min_val, max_val]\n",
    "\n",
    "model = nn.Sequential(\n",
    "      qnn.QuantHardTanh(act_quant=InputQuantizer),\n",
    "      QuantLinear(input_size, hidden1, bias=True, weight_bit_width=weight_bit_width),\n",
    "      nn.BatchNorm1d(hidden1),\n",
    "      nn.Dropout(0.5),\n",
    "      QuantReLU(bit_width=act_bit_width),\n",
    "      QuantLinear(hidden1, hidden2, bias=True, weight_bit_width=weight_bit_width),\n",
    "      nn.BatchNorm1d(hidden2),\n",
    "      nn.Dropout(0.5),\n",
    "      QuantReLU(bit_width=act_bit_width),\n",
    "      QuantLinear(hidden2, hidden3, bias=True, weight_bit_width=weight_bit_width),\n",
    "      nn.BatchNorm1d(hidden3),\n",
    "      nn.Dropout(0.5),\n",
    "      QuantReLU(bit_width=act_bit_width),\n",
    "      QuantLinear(hidden3, num_classes, bias=True, weight_bit_width=weight_bit_width)\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): QuantHardTanh(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): Identity()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ConstScaling(\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): Identity()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_init_module): Identity()\n",
       "            (value): StatelessBuffer()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): QuantConv1d(\n",
       "    2, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): QuantReLU(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): ReLU()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "            (stats_input_view_shape_impl): OverTensorView()\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsPercentile()\n",
       "            )\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "            (restrict_scaling): _RestrictValue(\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (clamp_scaling): _ClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "            )\n",
       "            (restrict_inplace_preprocess): Identity()\n",
       "            (restrict_preprocess): Identity()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): QuantConv1d(\n",
       "    128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): QuantReLU(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): ReLU()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "            (stats_input_view_shape_impl): OverTensorView()\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsPercentile()\n",
       "            )\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "            (restrict_scaling): _RestrictValue(\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (clamp_scaling): _ClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "            )\n",
       "            (restrict_inplace_preprocess): Identity()\n",
       "            (restrict_preprocess): Identity()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (7): Flatten(start_dim=1, end_dim=-1)\n",
       "  (8): QuantLinear(\n",
       "    in_features=16384, out_features=128, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): QuantReLU(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): ReLU()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "            (stats_input_view_shape_impl): OverTensorView()\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsPercentile()\n",
       "            )\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "            (restrict_scaling): _RestrictValue(\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (clamp_scaling): _ClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "            )\n",
       "            (restrict_inplace_preprocess): Identity()\n",
       "            (restrict_preprocess): Identity()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (11): QuantLinear(\n",
       "    in_features=128, out_features=128, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): QuantReLU(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): ReLU()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "            (stats_input_view_shape_impl): OverTensorView()\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsPercentile()\n",
       "            )\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "            (restrict_scaling): _RestrictValue(\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (clamp_scaling): _ClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "            )\n",
       "            (restrict_inplace_preprocess): Identity()\n",
       "            (restrict_preprocess): Identity()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (14): QuantLinear(\n",
       "    in_features=128, out_features=4, bias=True\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import IntBias\n",
    "from brevitas.inject.enum import ScalingImplType\n",
    "from brevitas.inject.defaults import Int8ActPerTensorFloatMinMaxInit\n",
    "\n",
    "# Adjustable hyperparameters\n",
    "input_bits = 4\n",
    "a_bits = 4\n",
    "w_bits = 4\n",
    "filters_conv = 64\n",
    "filters_dense = 128\n",
    "#batch_size = 1024\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "class InputQuantizer(Int8ActPerTensorFloatMinMaxInit):\n",
    "    bit_width = input_bits\n",
    "    min_val = -2.0\n",
    "    max_val = 2.0\n",
    "    scaling_impl_type = ScalingImplType.CONST # Fix the quantization range to [min_val, max_val]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # Input quantization layer\n",
    "    qnn.QuantHardTanh(act_quant=InputQuantizer),\n",
    "\n",
    "    qnn.QuantConv1d(2, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "\n",
    "    qnn.QuantConv1d(filters_conv, filters_conv, 3, padding=1, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_conv),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "\n",
    "    qnn.QuantLinear(filters_conv*128, filters_dense, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_dense),\n",
    "    qnn.QuantReLU(bit_width=a_bits),\n",
    "\n",
    "    qnn.QuantLinear(filters_dense, filters_dense, weight_bit_width=w_bits, bias=False),\n",
    "    nn.BatchNorm1d(filters_dense),\n",
    "    qnn.QuantReLU(bit_width=a_bits, return_quant_tensor=True),\n",
    "\n",
    "    qnn.QuantLinear(filters_dense, 4, weight_bit_width=w_bits, bias=True),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the MLP's output is not yet quantized. Even though we want the final output of our MLP to be a binary (0/1) value indicating the classification, we've only defined a single-neuron FC layer as the output. While training the network we'll pass that output through a sigmoid function as part of the loss criterion, which [gives better numerical stability](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). Later on, after we're done training the network, we'll add a quantization node at the end before we export it to FINN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train and Test  Methods  <a id='train_test'></a>\n",
    "The train and test methods will use a `DataLoader`, which feeds the model with a new predefined batch of training data in each iteration, until the entire training data is fed to the model. Each repetition of this process is called an `epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 0\n"
     ]
    }
   ],
   "source": [
    "# Select which GPU to use (if available)\n",
    "gpu = 0\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.device(gpu)\n",
    "    print(\"Using GPU %d\" % gpu)\n",
    "else:\n",
    "    gpu = None\n",
    "    print(\"Using CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "\n",
    "    for (inputs, target) in tqdm(train_loader, desc=\"Batches\", leave=False):   \n",
    "        if gpu is not None:\n",
    "            inputs = inputs.cuda()\n",
    "            target = target.cuda()\n",
    "                \n",
    "        # forward pass\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "           \n",
    "    return losses\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for (inputs, target) in test_loader:\n",
    "            if gpu is not None:\n",
    "                inputs = inputs.cuda()\n",
    "                target = target.cuda()\n",
    "            output = model(inputs)\n",
    "\n",
    "            pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "            target = target.cpu().float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred)\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80659b0c260540d1bb38709ab8b4fb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53286ec4c72f46789abc6cdf7cfacfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training loss = 1.777048, test accuracy = 0.831060\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719336087e994c5ca056d36ec69263b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss = 1.741661, test accuracy = 0.820703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232bcfda9bdf4b9e8477bddbef5084e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Training loss = 1.726277, test accuracy = 0.810877\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5452d295cdc249928d73308b7b30c791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "num_epochs = 5\n",
    "\n",
    "data_loader_train = train_loader\n",
    "data_loader_test = test_loader\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=1)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        loss_epoch = train(model, data_loader_train, optimizer, criterion)\n",
    "        test_acc = test(model, data_loader_test)\n",
    "        print(\"Epoch %d: Training loss = %f, test accuracy = %f\" % (epoch, np.mean(loss_epoch), test_acc))\n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)\n",
    "        lr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "max_pool1d() Invalid computed output size: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m        \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_f1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_precision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_recall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m    106\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 107\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, epoch)\u001b[0m\n\u001b[1;32m     62\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 65\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust shape for Conv1d\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Simulated target tensor (batch_size=1, single label)       \u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m  \u001b[38;5;66;03m# Ensure correct shape\u001b[39;00m\n\u001b[1;32m     71\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Converts shape from [batch_size, 1] to [batch_size]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/pooling.py:134\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:740\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    739\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: max_pool1d() Invalid computed output size: 0"
     ]
    }
   ],
   "source": [
    "# def train(model, train_loader, optimizer, criterion):\n",
    "#     losses = []\n",
    "#     # ensure model is in training mode\n",
    "#     model.train()    \n",
    "    \n",
    "#     for i, data in enumerate(train_loader, 0):        \n",
    "#         inputs, target = data\n",
    "#         inputs, target = inputs.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()   \n",
    "                \n",
    "#         # forward pass\n",
    "#         output = model(inputs.float())\n",
    "#         loss = criterion(output, target.unsqueeze(1))\n",
    "        \n",
    "#         # backward pass + run optimizer to update weights\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # keep track of loss value\n",
    "#         losses.append(loss.data.cpu().numpy()) \n",
    "           \n",
    "#     return losses\n",
    "\n",
    "# Initialize metrics\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "accuracy_func = MultilabelAccuracy(num_labels=4).to(device)\n",
    "precision_func = MultilabelPrecision(num_labels=4).to(device)\n",
    "recall_func = MultilabelRecall(num_labels=4).to(device)\n",
    "f1_score_func = MultilabelF1Score(num_labels=4).to(device)\n",
    "\n",
    "\n",
    "def train(model, train_loader, epoch=5):\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = epoch\n",
    "    best_accuracy = 0\n",
    "    patience = 20\n",
    "    counter = 0\n",
    "    \n",
    "    history_df = pd.DataFrame(columns=['epoch', 'accuracy','loss','precision', 'recall', 'F1-Score'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "       model.train()\n",
    "       running_loss = 0.0\n",
    "       correct = 0\n",
    "       total = 0\n",
    "       no_loops =0\n",
    "       precision =  0.0\n",
    "       recall = 0.0\n",
    "       f1 = 0.0\n",
    "       \n",
    "       \n",
    "\n",
    "       for inputs, labels in train_loader:\n",
    "           \n",
    "           inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(inputs)  # Adjust shape for Conv1d\n",
    "           # Simulated target tensor (batch_size=1, single label)       \n",
    "           \n",
    "           \n",
    "           \n",
    "            # Ensure correct shape\n",
    "           outputs = outputs.squeeze(1)  # Converts shape from [batch_size, 1] to [batch_size]\n",
    "           \n",
    "            # Compute loss\n",
    "           loss = criterion(outputs, labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           running_loss += loss.item()\n",
    "\n",
    "           # Calculate metrics \n",
    "           predictions = torch.sigmoid(outputs) > 0.5\n",
    "           \n",
    "           predictions = predictions.int()\n",
    "           \n",
    "           correct += accuracy_func(predictions, labels)\n",
    "           precision += precision_func(predictions, labels)\n",
    "           recall += recall_func(predictions, labels)\n",
    "           f1 += f1_score_func(predictions, labels)\n",
    "           \n",
    "           no_loops += 1 # to normalise preccision recall\n",
    "           total += labels.size(0) # total no of samples \n",
    "\n",
    "       \n",
    "\n",
    "       epoch_loss = running_loss / len(train_loader)\n",
    "       epoch_accuracy = correct /no_loops\n",
    "       epoch_recall = recall/no_loops\n",
    "       epoch_precision = precision/no_loops\n",
    "       epoch_f1 = f1/no_loops\n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "\n",
    "       print(f'''Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.8f}, Accuracy: {epoch_accuracy:.8f}, F1-Score: {epoch_f1}, Precision: {epoch_precision}, Recall: {epoch_recall}''')\n",
    "\n",
    "model.to(device)\n",
    "train(model, train_loader, epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 2, 3], expected input[256, 128, 2] to have 2 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 74\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;124mval_Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_epoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_epoch_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.8\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val_F1-Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_epoch_f1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_epoch_precision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_epoch_recall\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# # ensure model is in eval mode\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# model.eval() \u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# y_true = []\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# return accuracy_score(y_true, y_pred)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 19\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, test_loader)\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust shape for Conv1d\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Simulated target tensor (batch_size=1, single label)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m  \u001b[38;5;66;03m# Ensure correct shape\u001b[39;00m\n\u001b[1;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Converts shape from [batch_size, 1] to [batch_size]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/nn/quant_conv.py:105\u001b[0m, in \u001b[0;36mQuantConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Union[Tensor, QuantTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, QuantTensor]:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/nn/quant_layer.py:158\u001b[0m, in \u001b[0;36mQuantWeightBiasInputOutputLayer.forward_impl\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     quant_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m quant_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_quant(output_tensor)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_output(quant_output)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/nn/quant_conv.py:111\u001b[0m, in \u001b[0;36mQuantConv1d.inner_forward_impl\u001b[0;34m(self, x, quant_weight, quant_bias)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1d_same_zeros_pad_stride(x, quant_weight, quant_bias)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_bias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/quant_tensor/int_quant_tensor.py:50\u001b[0m, in \u001b[0;36mIntQuantTensor.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m INT_QUANT_TENSOR_FN_HANDLER:\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mINT_QUANT_TENSOR_FN_HANDLER\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m QUANT_TENSOR_FN_HANDLER:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m QUANT_TENSOR_FN_HANDLER[func](\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/quant_tensor/int_torch_handler.py:35\u001b[0m, in \u001b[0;36mconv1d_handler\u001b[0;34m(quant_input, quant_weight, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@implements_int_qt\u001b[39m(F\u001b[38;5;241m.\u001b[39mconv1d)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconv1d_handler\u001b[39m(quant_input, quant_weight, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 35\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mquant_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/brevitas/quant_tensor/int_torch_handler.py:154\u001b[0m, in \u001b[0;36mquant_layer\u001b[0;34m(fn, quant_input, quant_weight, bias, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m compute_output_quant_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(quant_input, IntQuantTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    151\u001b[0m     quant_weight, IntQuantTensor)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_unpack_quant_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\n\u001b[1;32m    162\u001b[0m         _unpack_quant_tensor(quant_input),\n\u001b[1;32m    163\u001b[0m         _unpack_quant_tensor(quant_weight),\n\u001b[1;32m    164\u001b[0m         _unpack_quant_tensor(bias),\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 2, 3], expected input[256, 128, 2] to have 2 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(model, test_loader):    \n",
    "\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_no_loops = 0\n",
    "    val_precision =  0.0\n",
    "    val_recall = 0.0\n",
    "    val_f1 = 0.0\n",
    "       \n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "           \n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(inputs.transpose(2,1))  # Adjust shape for Conv1d\n",
    "          # Simulated target tensor (batch_size=1, single label)\n",
    "       \n",
    "           # Ensure correct shape\n",
    "          outputs = outputs.squeeze(1)  # Converts shape from [batch_size, 1] to [batch_size]\n",
    "           \n",
    "           # Compute loss\n",
    "          loss = criterion(outputs, labels)\n",
    "          val_running_loss += loss.item()\n",
    "\n",
    "          # Calculate metrics \n",
    "          predictions = torch.sigmoid(outputs) > 0.5\n",
    "          \n",
    "          predictions = predictions.int()\n",
    "\n",
    "        \n",
    "        # Compute metrics\n",
    "          val_correct += accuracy_func(predictions, labels)\n",
    "          val_precision += precision_func(predictions, labels)\n",
    "          val_recall += recall_func(predictions, labels)\n",
    "          val_f1 += f1_score_func(predictions, labels)\n",
    "\n",
    "          \n",
    "          val_no_loops += 1 # to normalise preccision recall\n",
    "          val_total += labels.size(0) # total no of samples \n",
    "        \n",
    "    val_epoch_loss = val_running_loss / len(test_loader)\n",
    "    val_epoch_accuracy = val_correct / val_no_loops\n",
    "    val_epoch_recall = val_recall/val_no_loops\n",
    "    val_epoch_precision = val_precision/val_no_loops\n",
    "    val_epoch_f1 = val_f1/val_no_loops\n",
    "\n",
    "    print(f'''val_Loss: {val_epoch_loss:.8}, val_Accuracy: {val_epoch_accuracy:.8}, val_F1-Score: {val_epoch_f1}, Precision: {val_epoch_precision}, Recall: {val_epoch_recall}\\n\\n''')\n",
    "\n",
    "\n",
    "    # # ensure model is in eval mode\n",
    "    # model.eval() \n",
    "    # y_true = []\n",
    "    # y_pred = []\n",
    "   \n",
    "    # with torch.no_grad():\n",
    "    #     for data in test_loader:\n",
    "    #         inputs, target = data\n",
    "    #         inputs, target = inputs.to(device), target.to(device)\n",
    "    #         output_orig = model(inputs.float())\n",
    "    #         # run the output through sigmoid\n",
    "    #         output = torch.sigmoid(output_orig)  \n",
    "    #         # compare against a threshold of 0.5 to generate 0/1\n",
    "    #         pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "    #         target = target.cpu().float()\n",
    "    #         y_true.extend(target.tolist()) \n",
    "    #         y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    # return accuracy_score(y_true, y_pred)\n",
    "\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion):\n",
    "    losses = []\n",
    "    # ensure model is in training mode\n",
    "    model.train()    \n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):        \n",
    "        inputs, target = data\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        optimizer.zero_grad()   \n",
    "                \n",
    "        # forward pass\n",
    "        output = model(inputs.float())\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass + run optimizer to update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # keep track of loss value\n",
    "        losses.append(loss.data.cpu().numpy()) \n",
    "           \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            output_orig = model(inputs.float())\n",
    "            # run the output through sigmoid\n",
    "            output = torch.sigmoid(output_orig)  \n",
    "            # compare against a threshold of 0.5 to generate 0/1\n",
    "            pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "            target = target.cpu().float()\n",
    "            y_true.extend(target.tolist()) \n",
    "            y_pred.extend(pred)\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the QNN <a id=\"train_qnn\"></a>\n",
    "\n",
    "We provide two options for training below: you can opt for training the model from scratch (slower) or use a pre-trained model (faster). The first option will give more insight into how the training process works, while the second option will likely give better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option 1, slower) Train the Model from Scratch <a id=\"train_scratch\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training our MLP we need to define some hyperparameters. Moreover, in order to monitor the loss function evolution over epochs, we need to define a method for it. As mentioned earlier, we'll use a loss criterion which applies a sigmoid function during the training phase (`BCEWithLogitsLoss`). For the testing phase, we're manually computing the sigmoid and thresholding at 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.001 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss:   0%|          | 0/10 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m t \u001b[38;5;241m=\u001b[39m trange(num_epochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m t:\n\u001b[0;32m---> 16\u001b[0m         loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         test_acc \u001b[38;5;241m=\u001b[39m test(model, test_loader)\n\u001b[1;32m     18\u001b[0m         t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss = \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m test accuracy = \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(loss_epoch), test_acc))\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, criterion)\u001b[0m\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# keep track of loss value\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()) \n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "running_loss = []\n",
    "running_test_acc = []\n",
    "t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "for epoch in t:\n",
    "        loss_epoch = train(model, train_loader, optimizer,criterion)\n",
    "        test_acc = test(model, test_loader)\n",
    "        t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "        t.refresh() # to show immediately the update           \n",
    "        running_loss.append(loss_epoch)\n",
    "        running_test_acc.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Brevitas model to disk\n",
    "torch.save(model.state_dict(), \"state_dict_self-trained.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Surgery Before Export <a id=\"network_surgery\"></a>\n",
    "\n",
    "Sometimes, it's desirable to make some changes to our trained network prior to export (this is known in general as \"network surgery\"). This depends on the model and is not generally necessary, but in this case we want to make a couple of changes to get better results with FINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CPU before surgery\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by padding the input. Our input vectors are 593-bit, which will make folding (parallelization) for the first layer a bit tricky since 593 is a prime number. So we'll pad the weight matrix of the first layer with seven 0-valued columns to work with an input size of 600 instead. When using the modified network we'll similarly provide inputs padded to 600 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll modify the expected input/output ranges. In FINN, we prefer to work with bipolar {-1, +1} instead of binary {0, 1} values. To achieve this, we'll create a \"wrapper\" model that handles the pre/postprocessing as follows:\n",
    "\n",
    "* on the input side, we'll pre-process by (x + 1) / 2 in order to map incoming {-1, +1} inputs to {0, 1} ones which the trained network is used to. Since we're just multiplying/adding a scalar, these operations can be [*streamlined*](https://finn.readthedocs.io/en/latest/nw_prep.html#streamlining-transformations) by FINN and implemented with no extra cost.\n",
    "\n",
    "* on the output side, we'll add a binary quantizer which maps everthing below 0 to -1 and everything above 0 to +1. This is essentially the same behavior as the sigmoid we used earlier, except the outputs are bipolar instead of binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPForExport(\n",
       "  (pretrained): Sequential(\n",
       "    (0): QuantHardTanh(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): Identity()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "              (input_view_impl): Identity()\n",
       "            )\n",
       "            (scaling_impl): ConstScaling(\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): Identity()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_init_module): Identity()\n",
       "              (value): StatelessBuffer()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): QuantConv1d(\n",
       "      2, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "              (input_view_impl): Identity()\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): QuantConv1d(\n",
       "      64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "              (input_view_impl): Identity()\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Flatten(start_dim=1, end_dim=-1)\n",
       "    (8): QuantLinear(\n",
       "      in_features=8192, out_features=128, bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "              (input_view_impl): Identity()\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): QuantLinear(\n",
       "      in_features=128, out_features=128, bias=False\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (12): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "              (input_view_impl): Identity()\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): QuantLinear(\n",
       "      in_features=128, out_features=4, bias=True\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qnt_output): QuantIdentity(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): Identity()\n",
       "        (tensor_quant): ClampedBinaryQuant(\n",
       "          (scaling_impl): ConstScaling(\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_init_module): Identity()\n",
       "            (value): StatelessBuffer()\n",
       "          )\n",
       "          (bit_width): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "          (zero_point): StatelessBuffer()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (tensor_clamp_impl): TensorClamp()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "\n",
    "class MLPForExport(nn.Module):\n",
    "    def __init__(self, my_pretrained_model, dim=128):\n",
    "        super(MLPForExport, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.qnt_output = QuantIdentity(\n",
    "            quant_type='binary', \n",
    "            scaling_impl_type='const',\n",
    "            bit_width=1, min_val=-1.0, max_val=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x contains bipolar {-1,1} elems\n",
    "        # shift from {-1,1} -> {0,1} since that is the\n",
    "        # input range for the trained network\n",
    "        out_original = self.pretrained(x)\n",
    "        out_final = self.qnt_output(out_original)   # output as {-1,1}     \n",
    "        return out_final\n",
    "\n",
    "model_for_export = MLPForExport(model)\n",
    "model_for_export.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to QONNX and Conversion to FINN-ONNX <a id=\"export_qonnx\" ></a>\n",
    "\n",
    "\n",
    "[ONNX](https://onnx.ai/) is an open format built to represent machine learning models, and the FINN compiler expects an ONNX model as input. We'll now export our network into ONNX to be imported and used in FINN for the next notebooks. Note that the particular ONNX representation used for FINN differs from standard ONNX, you can read more about this [here](https://finn.readthedocs.io/en/latest/internals.html#intermediate-representation-finn-onnx).\n",
    "\n",
    "You can see below how we export a trained network in Brevitas into a FINN-compatible ONNX representation (QONNX). QONNX is the format we can export from Brevitas, to feed it into the FINN compiler, we will need to make a conversion to the FINN-ONNX format which is the intermediate representation the compiler works on. The conversion of the FINN-ONNX format is a FINN compiler transformation and to be able to apply it to our model, we will need to wrap it into [ModelWrapper](https://finn.readthedocs.io/en/latest/internals.html#modelwrapper). This is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model. Then we can call the conversion function to obtain the model in FINN-ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 128])\n",
      "Model saved to plutoCNN.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dspedia/.local/lib/python3.10/site-packages/qonnx/transformation/gemm_to_matmul.py:57: UserWarning: The GemmToMatMul transformation only offers explicit support for version 9 of the Gemm node, but the ONNX version of the supplied model is 17. Thus the transformation may fail or return incomplete results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.change_3d_tensors_to_4d import Change3DTo4DTensors\n",
    "from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "ready_model_filename = \"plutoCNN.onnx\"\n",
    "input_shape = (1,2, 128)\n",
    "\n",
    "#model_for_export = model\n",
    "\n",
    "#change_3d_tensors_to_4d.apply(model_for_export)\n",
    "\n",
    "# create a QuantTensor instance to mark input as bipolar during export\n",
    "input_a = np.random.randint(1, size=input_shape).astype(np.float32)\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "print(input_t.shape)\n",
    "\n",
    "#Move to CPU before export\n",
    "model_for_export.cpu()\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    model_for_export, export_path=ready_model_filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# clean-up\n",
    "qonnx_cleanup(ready_model_filename, out_file=ready_model_filename)\n",
    "\n",
    "# ModelWrapper\n",
    "outmodel = ModelWrapper(ready_model_filename)\n",
    "\n",
    "# Setting the input datatype explicitly because it doesn't get derived from the export function\n",
    "outmodel.set_tensor_datatype(outmodel.graph.input[0].name, DataType[\"INT8\"])\n",
    "outmodel = outmodel.transform(ConvertQONNXtoFINN())\n",
    "\n",
    "outmodel = outmodel.transform(Change3DTo4DTensors())\n",
    "\n",
    "outmodel.save(ready_model_filename)\n",
    "\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Exported ONNX in Netron\n",
    "\n",
    "Let's examine the exported ONNX model with [Netron](https://github.com/lutzroeder/netron), which is a visualizer for neural networks and allows interactive investigation of network properties. For example, you can click on the individual nodes and view the properties. Particular things of note:\n",
    "\n",
    "* The input tensor \"0\" is annotated with `quantization: finn_datatype: BIPOLAR`\n",
    "* The input preprocessing (x + 1) / 2 is exported as part of the network (initial `Add` and `Div` layers)\n",
    "* Brevitas `QuantLinear` layers are exported to ONNX as `MatMul`. We've exported the padded version; shape of the first MatMul node's weight parameter is 600x64\n",
    "* The weight parameters (second inputs) for MatMul nodes are annotated with `quantization: finn_datatype: INT2`\n",
    "* The quantized activations are exported as `MultiThreshold` nodes with `domain=qonnx.custom_op.general`\n",
    "* There's a final `MultiThreshold` node with threshold=0 to produce the final bipolar output (this is the `qnt_output` from `CybSecMLPForExport`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'plutoMLP.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x749e90066590>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it! <a id=\"thats_it\" ></a>\n",
    "You created, trained and tested a quantized MLP that is ready to be loaded into FINN, congratulations! You can now proceed to the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
