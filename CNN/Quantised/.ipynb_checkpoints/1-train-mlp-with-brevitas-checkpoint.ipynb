{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Quantized MLP on UNSW-NB15 with Brevitas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Live FINN tutorial:** We recommend clicking **Cell -> Run All** when you start reading this notebook for \"latency hiding\".</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will show how to create, train and export a quantized Multi Layer Perceptron (MLP) with quantized weights and activations with [Brevitas](https://github.com/Xilinx/brevitas).\n",
    "Specifically, the task at hand will be to label network packets as normal or suspicious (e.g. originating from an attacker, virus, malware or otherwise) by training on a quantized variant of the UNSW-NB15 dataset. \n",
    "\n",
    "**You won't need a GPU to train the neural net.** This MLP will be small enough to train on a modern x86 CPU, so no GPU is required to follow this tutorial  Alternatively, we provide pre-trained parameters for the MLP if you want to skip the training entirely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick introduction to the task and the dataset\n",
    "\n",
    "*The task:* The goal of [*network intrusion detection*](https://ieeexplore.ieee.org/abstract/document/283931) is to identify, preferably in real time, unauthorized use, misuse, and abuse of computer systems by both system insiders and external penetrators. This may be achieved by a mix of techniques, and machine-learning (ML) based techniques are increasing in popularity. \n",
    "\n",
    "*The dataset:* Several datasets are available for use in ML-based methods for intrusion detection.\n",
    "The **UNSW-NB15** is one such dataset created by the Australian Centre for Cyber Security (ACCS) to provide a comprehensive network based data set which can reflect modern network traffic scenarios. You can find more details about the dataset on [its homepage](https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/).\n",
    "\n",
    "*Performance considerations:* FPGAs are commonly used for implementing high-performance packet processing systems that still provide a degree of programmability. To avoid introducing bottlenecks on the network, the DNN implementation must be capable of detecting malicious ones at line rate, which can be millions of packets per second, and is expected to increase further as next-generation networking solutions provide increased\n",
    "throughput. This is a good reason to consider FPGA acceleration for this particular use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "-------------\n",
    "\n",
    "* [Load the UNSW_NB15 Dataset](#load_dataset) \n",
    "* [Define a PyTorch Device](#define_pytorch_device)\n",
    "* [Define the Quantized MLP Model](#define_quantized_mlp)\n",
    "* [Define Train and Test  Methods](#train_test)\n",
    "    * [(Option 1) Train the Model from Scratch](#train_scratch)\n",
    "    * [(Option 2) Load Pre-Trained Parameters](#load_pretrained)\n",
    "* [Network Surgery Before Export](#network_surgery)\n",
    "* [Export to QONNX and Conversion to FINN-ONNX](#export_qonnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import onnx\n",
    "import torch\n",
    "\n",
    "model_dir = \"\"#os.environ['FINN_ROOT'] + \"/notebooks/end2end_example/cybersecurity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is important -- always import onnx before torch**. This is a workaround for a [known bug](https://github.com/onnx/onnx/issues/2394)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the UNSW_NB15 Dataset <a id='load_dataset'></a>\n",
    "\n",
    "### Dataset Quantization <a id='dataset_qnt'></a>\n",
    "\n",
    "The goal of this notebook is to train a Quantized Neural Network (QNN) to be later deployed as an FPGA accelerator generated by the FINN compiler. Although we can choose a variety of different precisions for the input, [Murovic and Trost](https://ev.fe.uni-lj.si/1-2-2019/Murovic.pdf) have previously shown we can actually binarize the inputs and still get good (90%+) accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a binarized representation for the dataset by following the procedure defined by Murovic and Trost, which we repeat briefly here:\n",
    "\n",
    "* Original features have different formats ranging from integers, floating numbers to strings.\n",
    "* Integers, which for example represent a packet lifetime, are binarized with as many bits as to include the maximum value. \n",
    "* Another case is with features formatted as strings (protocols), which are binarized by simply counting the number of all different strings for each feature and coding them in the appropriate number of bits.\n",
    "* Floating-point numbers are reformatted into fixed-point representation.\n",
    "* In the end, each sample is transformed into a 593-bit wide binary vector. \n",
    "* All vectors are labeled as bad (0) or normal (1)\n",
    "\n",
    "Following Murovic and Trost's open-source implementation provided as a Matlab script [here](https://github.com/TadejMurovic/BNN_Deployment/blob/master/cybersecurity_dataset_unswb15.m), we've created a [Python version](dataloader_quantized.py).\n",
    "\n",
    "<font color=\"red\">**Live FINN tutorial:** Downloading the original dataset and quantizing it can take some time, so we provide a download link to the pre-quantized version for your convenience. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-05 19:28:48--  https://zenodo.org/record/4519767/files/unsw_nb15_binarized.npz?download=1\n",
      "188.185.48.194, 188.185.43.25, 188.185.45.92, ...\n",
      "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 MOVED PERMANENTLY\n",
      "Location: /records/4519767/files/unsw_nb15_binarized.npz [following]\n",
      "--2025-03-05 19:28:48--  https://zenodo.org/records/4519767/files/unsw_nb15_binarized.npz\n",
      "Reusing existing connection to zenodo.org:443.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 13391907 (13M) [application/octet-stream]\n",
      "Saving to: ‘unsw_nb15_binarized.npz’\n",
      "\n",
      "unsw_nb15_binarized 100%[===================>]  12.77M  5.73MB/s    in 2.2s    \n",
      "\n",
      "2025-03-05 19:28:51 (5.73 MB/s) - ‘unsw_nb15_binarized.npz’ saved [13391907/13391907]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O unsw_nb15_binarized.npz https://zenodo.org/record/4519767/files/unsw_nb15_binarized.npz?download=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the binarized numpy arrays from the .npz archive and wrap them as a PyTorch `TensorDataset` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_array(arr):\n",
    "\n",
    "  # find the minimum and maximum values in the array.\n",
    "  min_val = np.min(arr)\n",
    "  max_val = np.max(arr)\n",
    "\n",
    "  normalized_arr = 2 * (arr - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "  return normalized_arr\n",
    "\n",
    "\n",
    "def filter_strings(lst):\n",
    "    filtered_list = [s for s in lst if not any(digit in s for digit in \"23456789\")]\n",
    "    return filtered_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array min: -2.0, max: 2.0\n",
      "Normalized array min: -1.0, max: 1.0\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from torch.utils.data import TensorDataset\n",
    "\n",
    "# def get_preqnt_dataset(data_dir: str, train: bool):\n",
    "#     unsw_nb15_data = np.load(data_dir + \"/unsw_nb15_binarized.npz\")\n",
    "#     if train:\n",
    "#         partition = \"train\"\n",
    "#     else:\n",
    "#         partition = \"test\"\n",
    "#     part_data = unsw_nb15_data[partition].astype(np.float32)\n",
    "#     part_data = torch.from_numpy(part_data)\n",
    "#     part_data_in = part_data[:, :-1]\n",
    "#     part_data_out = part_data[:, -1]\n",
    "#     return TensorDataset(part_data_in, part_data_out)\n",
    "\n",
    "# train_quantized_dataset = get_preqnt_dataset(\".\", True)\n",
    "# test_quantized_dataset = get_preqnt_dataset(\".\", False)\n",
    "\n",
    "# print(\"Samples in each set: train = %d, test = %s\" % (len(train_quantized_dataset), len(test_quantized_dataset))) \n",
    "# print(\"Shape of one input sample: \" +  str(train_quantized_dataset[0][0].shape))\n",
    "\n",
    "import numpy as np\n",
    "import os as os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "#from torchsummary import summary\n",
    "import brevitas.nn as qnn\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder = \"../../../PlutoImport\"\n",
    "files = os.listdir(folder)\n",
    "\n",
    "filtered_files = filter_strings(files)\n",
    "\n",
    "factor = 8\n",
    "noFiles = len(filtered_files)\n",
    "\n",
    "arr = np.ndarray((int(7800*noFiles/factor),128*factor,2), float)\n",
    "labels = np.ndarray((int(7800*noFiles/factor),4))\n",
    "\n",
    "seed = 42\n",
    "\n",
    "i = 0;\n",
    "for idx, npz in enumerate(filtered_files):\n",
    "    \n",
    "    a = np.load(os.path.join(folder, npz))\n",
    "    \n",
    "    start_idx = (idx*int(7800/factor)) if idx <20 else (idx)*int(7800/factor)-1\n",
    "    end_idx = (1+idx)*int(7800/factor) if idx <20 else (1+idx)*int(7800/factor)-1\n",
    "    \n",
    "    # print(f\"start index: {start_idx}, end index {end_idx}, activate channels: {a['active_channels']}\")\n",
    "       \n",
    "    reshaped_arr = a[\"samples\"].reshape(int(7800/factor), 128*factor)\n",
    "    \n",
    "    float_array = np.stack((reshaped_arr.real, reshaped_arr.imag), axis=-1) \n",
    "\n",
    "    arr[start_idx:end_idx] = float_array\n",
    "    labels[start_idx:end_idx] = np.tile(a[\"active_channels\"],  (int(7800/factor), 1))\n",
    "\n",
    "    i+=1\n",
    "    if i >= noFiles:\n",
    "        break\n",
    "    \n",
    "normalized_array = normalize_array(arr)\n",
    "labels = labels[:-1]\n",
    "\n",
    "\n",
    "print(f\"Original array min: {np.min(arr)}, max: {np.max(arr)}\")\n",
    "print(f\"Normalized array min: {np.min(normalized_array)}, max: {np.max(normalized_array)}\")\n",
    "\n",
    "arr = normalized_array[:-1]\n",
    "\n",
    "# first split into train+val and test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(arr, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "# then split train+val into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=seed)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/frank/OneDrive/Strathclyde/Strathclyde/Year5/Project/colab/CoRSoC/CNN/Quantised'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up DataLoader\n",
    "\n",
    "Following either option, we now have access to the quantized dataset. We will wrap the dataset in a PyTorch `DataLoader` for easier access in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# batch_size = 1000\n",
    "\n",
    "# # dataset loaders\n",
    "# train_quantized_loader = DataLoader(train_quantized_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_quantized_loader = DataLoader(test_quantized_dataset, batch_size=batch_size, shuffle=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape for 1 batch: torch.Size([1000, 593])\n",
      "Label shape for 1 batch: torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for x,y in train_quantized_loader:\n",
    "    print(\"Input shape for 1 batch: \" + str(x.shape))\n",
    "    print(\"Label shape for 1 batch: \" + str(y.shape))\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a PyTorch Device <a id='define_pytorch_device'></a> \n",
    "\n",
    "GPUs can significantly speed-up training of deep neural networks. We check for availability of a GPU and if so define it as target device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Target device: \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Quantized MLP Model <a id='define_quantized_mlp'></a>\n",
    "\n",
    "We'll now define an MLP model that will be trained to perform inference with quantized weights and activations.\n",
    "For this, we'll use the quantization-aware training (QAT) capabilities offered by [Brevitas](https://github.com/Xilinx/brevitas).\n",
    "\n",
    "Our MLP will have four fully-connected (FC) layers in total: three hidden layers with 64 neurons, and a final output layer with a single output, all using 2-bit weights. We'll use 2-bit quantized ReLU activation functions, and apply batch normalization between each FC layer and its activation.\n",
    "\n",
    "In case you'd like to experiment with different quantization settings or topology parameters, we'll define all these topology settings as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 593      \n",
    "hidden1 = 64      \n",
    "hidden2 = 64\n",
    "hidden3 = 64\n",
    "weight_bit_width = 2\n",
    "act_bit_width = 2\n",
    "num_classes = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our MLP using the layer primitives provided by Brevitas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantCNNModel(\n",
       "  (conv1): QuantConv1d(\n",
       "    2, 16, kernel_size=(2,), stride=(1,)\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (conv2): QuantConv1d(\n",
       "    16, 16, kernel_size=(3,), stride=(1,)\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (pool1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (conv3): QuantConv1d(\n",
       "    16, 32, kernel_size=(5,), stride=(1,)\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (conv4): QuantConv1d(\n",
       "    32, 32, kernel_size=(5,), stride=(1,)\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (pool2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (relu): QuantReLU(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): ReLU()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClamp()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "            (stats_input_view_shape_impl): OverTensorView()\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsPercentile()\n",
       "            )\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "            (restrict_scaling): _RestrictValue(\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (clamp_scaling): _ClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "            )\n",
       "            (restrict_inplace_preprocess): Identity()\n",
       "            (restrict_preprocess): Identity()\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense1): QuantLinear(\n",
       "    in_features=8032, out_features=64, bias=True\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (out): QuantLinear(\n",
       "    in_features=64, out_features=1, bias=True\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (input_view_impl): Identity()\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "            (restrict_scaling_impl): FloatRestrictValue()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantLinear, QuantReLU\n",
    "import brevitas.nn as qnn\n",
    "import torch.nn as nn\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class QuantCNNModel(nn.Module):\n",
    "    def __init__(self, dim=128, n_channels=2, n_classes=1, width = 2):\n",
    "        super(QuantCNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = qnn.QuantConv1d(in_channels=n_channels, out_channels=16, kernel_size=2, bias=True, weight_bit_width=width)\n",
    "        self.conv2 = qnn.QuantConv1d(16, 16, kernel_size=3, bias=True, weight_bit_width=width)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.conv3 = qnn.QuantConv1d(16, 32, kernel_size=5, bias=True, weight_bit_width=width)\n",
    "        self.conv4 = qnn.QuantConv1d(32, 32, kernel_size=5, bias=True, weight_bit_width=width)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = qnn.QuantReLU(weight_bit_width=width)\n",
    "        self.dense1 = qnn.QuantLinear(8032, 64, bias=True, weight_bit_width=width)\n",
    "        self.out = qnn.QuantLinear(64, n_classes, bias=True, weight_bit_width=width)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self._get_conv_output(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model = QuantCNNModel()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the MLP's output is not yet quantized. Even though we want the final output of our MLP to be a binary (0/1) value indicating the classification, we've only defined a single-neuron FC layer as the output. While training the network we'll pass that output through a sigmoid function as part of the loss criterion, which [gives better numerical stability](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html). Later on, after we're done training the network, we'll add a quantization node at the end before we export it to FINN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Train and Test  Methods  <a id='train_test'></a>\n",
    "The train and test methods will use a `DataLoader`, which feeds the model with a new predefined batch of training data in each iteration, until the entire training data is fed to the model. Each repetition of this process is called an `epoch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_loader, optimizer, criterion):\n",
    "#     losses = []\n",
    "#     # ensure model is in training mode\n",
    "#     model.train()    \n",
    "    \n",
    "#     for i, data in enumerate(train_loader, 0):        \n",
    "#         inputs, target = data\n",
    "#         inputs, target = inputs.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()   \n",
    "                \n",
    "#         # forward pass\n",
    "#         output = model(inputs.float())\n",
    "#         loss = criterion(output, target.unsqueeze(1))\n",
    "        \n",
    "#         # backward pass + run optimizer to update weights\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # keep track of loss value\n",
    "#         losses.append(loss.data.cpu().numpy()) \n",
    "           \n",
    "#     return losses\n",
    "\n",
    "# Initialize metrics\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "accuracy_func = MultilabelAccuracy(num_labels=4).to(device)\n",
    "precision_func = MultilabelPrecision(num_labels=4).to(device)\n",
    "recall_func = MultilabelRecall(num_labels=4).to(device)\n",
    "f1_score_func = MultilabelF1Score(num_labels=4).to(device)\n",
    "\n",
    "\n",
    "def train(model, train_loader, epoch=5):\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = epoch\n",
    "    best_accuracy = 0\n",
    "    patience = 20\n",
    "    counter = 0\n",
    "    \n",
    "    history_df = pd.DataFrame(columns=['epoch', 'accuracy','loss','precision', 'recall', 'F1-Score'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "       model.train()\n",
    "       running_loss = 0.0\n",
    "       correct = 0\n",
    "       total = 0\n",
    "       no_loops =0\n",
    "       precision =  0.0\n",
    "       recall = 0.0\n",
    "       f1 = 0.0\n",
    "       \n",
    "       \n",
    "\n",
    "       for inputs, labels in train_loader:\n",
    "           \n",
    "           inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(inputs.transpose(2,1))  # Adjust shape for Conv1d\n",
    "           # Simulated target tensor (batch_size=1, single label)       \n",
    "           \n",
    "           \n",
    "           \n",
    "            # Ensure correct shape\n",
    "           outputs = outputs.squeeze(1)  # Converts shape from [batch_size, 1] to [batch_size]\n",
    "           \n",
    "            # Compute loss\n",
    "           loss = criterion(outputs, labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           running_loss += loss.item()\n",
    "\n",
    "           # Calculate metrics \n",
    "           predictions = torch.sigmoid(outputs) > 0.5\n",
    "           \n",
    "           predictions = predictions.int()\n",
    "           \n",
    "           correct += accuracy_func(predictions, labels)\n",
    "           precision += precision_func(predictions, labels)\n",
    "           recall += recall_func(predictions, labels)\n",
    "           f1 += f1_score_func(predictions, labels)\n",
    "           \n",
    "           no_loops += 1 # to normalise preccision recall\n",
    "           total += labels.size(0) # total no of samples \n",
    "\n",
    "       \n",
    "\n",
    "       epoch_loss = running_loss / len(train_loader)\n",
    "       epoch_accuracy = correct /no_loops\n",
    "       epoch_recall = recall/no_loops\n",
    "       epoch_precision = precision/no_loops\n",
    "       epoch_f1 = f1/no_loops\n",
    "       \n",
    "       \n",
    "       \n",
    "       \n",
    "\n",
    "       print(f'''Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.8f}, Accuracy: {epoch_accuracy:.8f}, F1-Score: {epoch_f1}, Precision: {epoch_precision}, Recall: {epoch_recall}''')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def test(model, test_loader):    \n",
    "\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_no_loops = 0\n",
    "    val_precision =  0.0\n",
    "    val_recall = 0.0\n",
    "    val_f1 = 0.0\n",
    "       \n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "           \n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(inputs.transpose(2,1))  # Adjust shape for Conv1d\n",
    "          # Simulated target tensor (batch_size=1, single label)\n",
    "       \n",
    "           # Ensure correct shape\n",
    "          outputs = outputs.squeeze(1)  # Converts shape from [batch_size, 1] to [batch_size]\n",
    "           \n",
    "           # Compute loss\n",
    "          loss = criterion(outputs, labels)\n",
    "          val_running_loss += loss.item()\n",
    "\n",
    "          # Calculate metrics \n",
    "          predictions = torch.sigmoid(outputs) > 0.5\n",
    "          \n",
    "          predictions = predictions.int()\n",
    "\n",
    "        \n",
    "        # Compute metrics\n",
    "          val_correct += accuracy_func(predictions, labels)\n",
    "          val_precision += precision_func(predictions, labels)\n",
    "          val_recall += recall_func(predictions, labels)\n",
    "          val_f1 += f1_score_func(predictions, labels)\n",
    "\n",
    "          \n",
    "          val_no_loops += 1 # to normalise preccision recall\n",
    "          val_total += labels.size(0) # total no of samples \n",
    "        \n",
    "    val_epoch_loss = val_running_loss / len(test_loader)\n",
    "    val_epoch_accuracy = val_correct / val_no_loops\n",
    "    val_epoch_recall = val_recall/val_no_loops\n",
    "    val_epoch_precision = val_precision/val_no_loops\n",
    "    val_epoch_f1 = val_f1/val_no_loops\n",
    "\n",
    "    print(f'''val_Loss: {val_epoch_loss:.8}, val_Accuracy: {val_epoch_accuracy:.8}, val_F1-Score: {val_epoch_f1}, Precision: {val_epoch_precision}, Recall: {val_epoch_recall}\\n\\n''')\n",
    "\n",
    "\n",
    "    # # ensure model is in eval mode\n",
    "    # model.eval() \n",
    "    # y_true = []\n",
    "    # y_pred = []\n",
    "   \n",
    "    # with torch.no_grad():\n",
    "    #     for data in test_loader:\n",
    "    #         inputs, target = data\n",
    "    #         inputs, target = inputs.to(device), target.to(device)\n",
    "    #         output_orig = model(inputs.float())\n",
    "    #         # run the output through sigmoid\n",
    "    #         output = torch.sigmoid(output_orig)  \n",
    "    #         # compare against a threshold of 0.5 to generate 0/1\n",
    "    #         pred = (output.detach().cpu().numpy() > 0.5) * 1\n",
    "    #         target = target.cpu().float()\n",
    "    #         y_true.extend(target.tolist()) \n",
    "    #         y_pred.extend(pred.reshape(-1).tolist())\n",
    "        \n",
    "    # return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the QNN <a id=\"train_qnn\"></a>\n",
    "\n",
    "We provide two options for training below: you can opt for training the model from scratch (slower) or use a pre-trained model (faster). The first option will give more insight into how the training process works, while the second option will likely give better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option 1, slower) Train the Model from Scratch <a id=\"train_scratch\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training our MLP we need to define some hyperparameters. Moreover, in order to monitor the loss function evolution over epochs, we need to define a method for it. As mentioned earlier, we'll use a loss criterion which applies a sigmoid function during the training phase (`BCEWithLogitsLoss`). For the testing phase, we're manually computing the sigmoid and thresholding at 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "lr = 0.001 \n",
    "\n",
    "def display_loss_plot(losses, title=\"Training loss\", xlabel=\"Iterations\", ylabel=\"Loss\"):\n",
    "    x_axis = [i for i in range(len(losses))]\n",
    "    plt.plot(x_axis,losses)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss criterion and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# running_loss = []\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# running_test_acc = []\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# t = trange(num_epochs, desc=\"Training loss\", leave=True)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m loss_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(model, train_quantized_loader, epoch \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     15\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m test(model, test_quantized_loader)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# t.refresh() # to show immediately the update           \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# running_loss.append(loss_epoch)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# running_test_acc.append(test_acc)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# running_loss = []\n",
    "# running_test_acc = []\n",
    "# t = trange(num_epochs, desc=\"Training loss\", leave=True)\n",
    "\n",
    "\n",
    "loss_epoch = train(model, train_quantized_loader, epoch =5)\n",
    "test_acc = test(model, test_quantized_loader)\n",
    "# t.set_description(\"Training loss = %f test accuracy = %f\" % (np.mean(loss_epoch), test_acc))\n",
    "# t.refresh() # to show immediately the update           \n",
    "# running_loss.append(loss_epoch)\n",
    "# running_test_acc.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_per_epoch = [np.mean(loss_per_epoch) for loss_per_epoch in running_loss]\n",
    "display_loss_plot(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc_per_epoch = [np.mean(acc_per_epoch) for acc_per_epoch in running_test_acc]\n",
    "display_loss_plot(acc_per_epoch, title=\"Test accuracy\", ylabel=\"Accuracy [%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065758149929554"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'brevitas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#from torchsummary import summary\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbrevitas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqnn\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCNNModel\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'brevitas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os as os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "#from torchsummary import summary\n",
    "import brevitas.nn as qnn\n",
    "from torchmetrics.classification import MultilabelAccuracy, MultilabelPrecision, MultilabelRecall, MultilabelF1Score\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, dim=128, n_channels=2, n_classes=1):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=n_channels, out_channels=16, kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(16, 16, kernel_size=3)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv1d(16, 32, kernel_size=5)\n",
    "        self.conv4 = nn.Conv1d(32, 32, kernel_size=5)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense1 = nn.Linear(8032, 64)\n",
    "        self.out = nn.Linear(64, n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self._get_conv_output(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class QuantCNNModel(nn.Module):\n",
    "    def __init__(self, dim=128, n_channels=2, n_classes=1, width = 2):\n",
    "        super(QuantCNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = qnn.QuantConv1d(in_channels=n_channels, out_channels=16, kernel_size=2, bias=True, weight_bit_width=width)\n",
    "        self.conv2 = qnn.QuantConv1d(16, 16, kernel_size=3, bias=True, weight_bit_width=width)\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.conv3 = qnn.QuantConv1d(16, 32, kernel_size=5, bias=True, weight_bit_width=width)\n",
    "        self.conv4 = qnn.QuantConv1d(32, 32, kernel_size=5, bias=True, weight_bit_width=width)\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = qnn.QuantReLU(weight_bit_width=width)\n",
    "        self.dense1 = qnn.QuantLinear(8032, 64, bias=True, weight_bit_width=width)\n",
    "        self.out = qnn.QuantLinear(64, n_classes, bias=True, weight_bit_width=width)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self._get_conv_output(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.dense1(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def trainModel(epoch = 1):\n",
    "    \n",
    "    results = os.path.expanduser(\"./results/\")  # Creates directory in the user's home directory\n",
    "    if not os.path.isdir(results):\n",
    "        os.mkdir(results)\n",
    "        os.chmod(results, 0o777)  # Sets full permissions\n",
    "    \n",
    "    seed = 42\n",
    "   \n",
    "    ############################\n",
    "    folder = \"../../../PlutoImport\"\n",
    "    files = os.listdir(folder)\n",
    "\n",
    "    filtered_files = filter_strings(files)\n",
    " \n",
    "    factor = 8\n",
    "    noFiles = len(filtered_files)\n",
    "\n",
    "    arr = np.ndarray((int(7800*noFiles/factor),128*factor,2), float)\n",
    "    labels = np.ndarray((int(7800*noFiles/factor),4))\n",
    "\n",
    "    i = 0;\n",
    "    for idx, npz in enumerate(filtered_files):\n",
    "        \n",
    "        a = np.load(os.path.join(folder, npz))\n",
    "        \n",
    "        start_idx = (idx*int(7800/factor)) if idx <20 else (idx)*int(7800/factor)-1\n",
    "        end_idx = (1+idx)*int(7800/factor) if idx <20 else (1+idx)*int(7800/factor)-1\n",
    "        \n",
    "        # print(f\"start index: {start_idx}, end index {end_idx}, activate channels: {a['active_channels']}\")\n",
    "           \n",
    "        reshaped_arr = a[\"samples\"].reshape(int(7800/factor), 128*factor)\n",
    "        \n",
    "        float_array = np.stack((reshaped_arr.real, reshaped_arr.imag), axis=-1) \n",
    "\n",
    "        arr[start_idx:end_idx] = float_array\n",
    "        labels[start_idx:end_idx] = np.tile(a[\"active_channels\"],  (int(7800/factor), 1))\n",
    "\n",
    "        i+=1\n",
    "        if i >= noFiles:\n",
    "            break\n",
    "        \n",
    "    normalized_array = normalize_array(arr)\n",
    "    labels = labels[:-1]\n",
    "    \n",
    "\n",
    "    print(f\"Original array min: {np.min(arr)}, max: {np.max(arr)}\")\n",
    "    print(f\"Normalized array min: {np.min(normalized_array)}, max: {np.max(normalized_array)}\")\n",
    "\n",
    "    arr = normalized_array[:-1]\n",
    "\n",
    "    # first split into train+val and test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(arr, labels, test_size=0.2, random_state=seed)\n",
    "\n",
    "    # then split train+val into train and val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=seed)\n",
    "\n",
    "\n",
    "#####################################\n",
    "\n",
    "    modelType = \"pluto\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Target device: \" + str(device))\n",
    "   # Instantiate the model\n",
    "    model = QuantCNNModel(dim=1024, n_channels=2, n_classes=4)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "   # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "   # Create DataLoader for training\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = epoch\n",
    "    best_accuracy = 0\n",
    "    patience = 20\n",
    "    counter = 0\n",
    "    \n",
    "    # Initialize metrics\n",
    "    accuracy_func = MultilabelAccuracy(num_labels=4).to(device)\n",
    "    precision_func = MultilabelPrecision(num_labels=4).to(device)\n",
    "    recall_func = MultilabelRecall(num_labels=4).to(device)\n",
    "    f1_score_func = MultilabelF1Score(num_labels=4).to(device)\n",
    "    \n",
    "    print(model)\n",
    "    #summary(model,(2,dim))\n",
    "    \n",
    "    history_df = pd.DataFrame(columns=['epoch', 'accuracy','loss','precision', 'recall', 'F1-Score'])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "       model.train()\n",
    "       running_loss = 0.0\n",
    "       correct = 0\n",
    "       total = 0\n",
    "       no_loops =0\n",
    "       precision =  0.0\n",
    "       recall = 0.0\n",
    "       f1 = 0.0\n",
    "       \n",
    "       val_running_loss = 0.0\n",
    "       val_correct = 0\n",
    "       val_total = 0\n",
    "       val_no_loops = 0\n",
    "       val_precision =  0.0\n",
    "       val_recall = 0.0\n",
    "       val_f1 = 0.0\n",
    "\n",
    "       for inputs, labels in train_loader:\n",
    "           \n",
    "           inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "           optimizer.zero_grad()\n",
    "           outputs = model(inputs.transpose(2,1))  # Adjust shape for Conv1d\n",
    "           # Simulated target tensor (batch_size=1, single label)       \n",
    "           \n",
    "           \n",
    "           \n",
    "            # Ensure correct shape\n",
    "           outputs = outputs.squeeze(1)  # Converts shape from [batch_size, 1] to [batch_size]\n",
    "           \n",
    "            # Compute loss\n",
    "           loss = criterion(outputs, labels)\n",
    "           loss.backward()\n",
    "           optimizer.step()\n",
    "           running_loss += loss.item()\n",
    "\n",
    "           # Calculate metrics \n",
    "           predictions = torch.sigmoid(outputs) > 0.5\n",
    "           \n",
    "           predictions = predictions.int()\n",
    "           \n",
    "           correct += accuracy_func(predictions, labels)\n",
    "           precision += precision_func(predictions, labels)\n",
    "           recall += recall_func(predictions, labels)\n",
    "           f1 += f1_score_func(predictions, labels)\n",
    "           \n",
    "           no_loops += 1 # to normalise preccision recall\n",
    "           total += labels.size(0) # total no of samples \n",
    "    \n",
    "       for inputs, labels in test_loader:\n",
    "          inputs, labels = inputs.to(device), labels.to(device)\n",
    "           \n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(inputs.transpose(2,1))  # Adjust shape for Conv1d\n",
    "          # Simulated target tensor (batch_size=1, single label)\n",
    "          \n",
    "          \n",
    "          \n",
    "           # Ensure correct shape\n",
    "          outputs = outputs.squeeze(1)  # Converts shape from [batch_size, 1] to [batch_size]\n",
    "           \n",
    "           # Compute loss\n",
    "          loss = criterion(outputs, labels)\n",
    "          val_running_loss += loss.item()\n",
    "\n",
    "          # Calculate metrics \n",
    "          predictions = torch.sigmoid(outputs) > 0.5\n",
    "          \n",
    "          predictions = predictions.int()\n",
    "\n",
    "        \n",
    "        # Compute metrics\n",
    "          val_correct += accuracy_func(predictions, labels)\n",
    "          val_precision += precision_func(predictions, labels)\n",
    "          val_recall += recall_func(predictions, labels)\n",
    "          val_f1 += f1_score_func(predictions, labels)\n",
    "\n",
    "          \n",
    "          val_no_loops += 1 # to normalise preccision recall\n",
    "          val_total += labels.size(0) # total no of samples \n",
    "\n",
    "       epoch_loss = running_loss / len(train_loader)\n",
    "       epoch_accuracy = correct /no_loops\n",
    "       epoch_recall = recall/no_loops\n",
    "       epoch_precision = precision/no_loops\n",
    "       epoch_f1 = f1/no_loops\n",
    "       \n",
    "       \n",
    "       val_epoch_loss = val_running_loss / len(test_loader)\n",
    "       val_epoch_accuracy = val_correct / val_no_loops\n",
    "       val_epoch_recall = val_recall/val_no_loops\n",
    "       val_epoch_precision = val_precision/val_no_loops\n",
    "       val_epoch_f1 = val_f1/val_no_loops\n",
    "       \n",
    "\n",
    "       print(f'''Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.8f}, Accuracy: {epoch_accuracy:.8f}, F1-Score: {epoch_f1}, Precision: {epoch_precision}, Recall: {epoch_recall}\n",
    "             val_Loss: {val_epoch_loss:.8}, val_Accuracy: {val_epoch_accuracy:.8}, val_F1-Score: {val_epoch_f1}, Precision: {val_epoch_precision}, Recall: {val_epoch_recall}\\n\\n''')\n",
    "\n",
    "\n",
    "      # history_df.loc[epoch] = [epoch, val_epoch_accuracy, val_epoch_loss, val_epoch_precision, val_epoch_recall, val_epoch_f1]\n",
    "       #Save the model if it's the best so far\n",
    "       if epoch_accuracy > best_accuracy:\n",
    "           best_accuracy = epoch_accuracy\n",
    "           torch.save(model.state_dict(), f'./results/best_model_{modelType}.pt')\n",
    "           counter = 0  # Reset patience counter\n",
    "       else:\n",
    "           counter += 1\n",
    "           if counter >= patience:\n",
    "               print(\"Early stopping triggered.\")\n",
    "               break\n",
    "    \n",
    "    return model\n",
    "   # Save training history to CSV\n",
    "    #history_df.to_csv(f'./results/training_history_{modelType}.csv', index=False)\n",
    "    \n",
    "    #testModel(modelType, f'./results/best_model_{modelType}.pt')\n",
    "\n",
    "def testModel(modelType, modelPath):\n",
    "\n",
    "    #Open dataset .h5 file either for training or testing\n",
    "    dset_fp = './sdr_wifi_' + \"test\" + '.hdf5'\n",
    "    dset = h5py.File(dset_fp, 'r')\n",
    "    X = dset['X'][()]\n",
    "    y = dset['y'][()]\n",
    "    dim = X.shape[1]\n",
    "    \n",
    "    #Load a pretrained model\n",
    "    model = CNNModel(dim=dim, n_channels=2, n_classes=1)\n",
    "\n",
    "    # Load the saved model state dictionary\n",
    "    model.load_state_dict(torch.load(modelPath))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create DataLoader for training\n",
    "    test_dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "     # Training loop\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    no_loops =0\n",
    "    precision =  0.0\n",
    "    recall = 0.0\n",
    "    f1 = 0.0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs.transpose(2,1))  # Adjust shape for Conv1d\n",
    "        # Simulated target tensor (batch_size=1, single label)\n",
    "         \n",
    "         # Ensure correct shape\n",
    "        outputs = outputs.squeeze(1)  # Converts shape from [batch_size, 1] to [batch_size]\n",
    "         \n",
    "         # Compute loss\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate metrics \n",
    "        predictions = torch.sigmoid(outputs) > 0.5\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        precision += calculate_metrics(labels, predictions)[\"precision\"]\n",
    "        recall += calculate_metrics(labels, predictions)[\"recall\"]\n",
    "        f1 += calculate_metrics(labels, predictions)[\"f1_score\"]\n",
    "        \n",
    "        no_loops += 1\n",
    "        total += labels.size(0)\n",
    "     \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_accuracy = correct / total\n",
    "    test_recall = recall/no_loops\n",
    "    test_precision = precision/no_loops\n",
    "    test_f1 = f1/no_loops\n",
    "  \n",
    "    print(f'Testing; Loss: {test_loss}, Accuracy: {test_accuracy}, Recall: {test_recall}, Precision: {test_precision}, F1-Score: {test_f1}')\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1-score for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (list/array): Ground truth labels\n",
    "    y_pred (list/array): Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing precision, recall, and F1-score\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    \n",
    "    # Count TP, FP, FN\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == pred == 1:\n",
    "            true_positive += 1\n",
    "        elif true == 0 and pred == 1:\n",
    "            false_positive += 1\n",
    "        elif true == 1 and pred == 0:\n",
    "            false_negative += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_array(arr):\n",
    "\n",
    "  # find the minimum and maximum values in the array.\n",
    "  min_val = np.min(arr)\n",
    "  max_val = np.max(arr)\n",
    "\n",
    "  normalized_arr = 2 * (arr - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "  return normalized_arr\n",
    "\n",
    "\n",
    "def filter_strings(lst):\n",
    "    filtered_list = [s for s in lst if not any(digit in s for digit in \"23456789\")]\n",
    "    return filtered_list\n",
    "\n",
    "model = trainModel(1)\n",
    "#QauntCNNModel(1024,2,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Brevitas model to disk\n",
    "torch.save(model.state_dict(), \"state_dict_self-trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Option 2, faster) Load Pre-Trained Parameters <a id=\"load_pretrained\"></a>\n",
    "\n",
    "Instead of training from scratch, you can also use pre-trained parameters we provide here. These parameters should achieve ~91.9% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Make sure the model is on CPU before loading a pretrained state_dict\n",
    "model = model.cpu()\n",
    "\n",
    "# Load pretrained weights\n",
    "trained_state_dict = torch.load(model_dir + \"/state_dict.pth\")[\"models_state_dict\"][0]\n",
    "\n",
    "model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model back to it's target device\n",
    "model.to(device)\n",
    "\n",
    "# Test for accuracy\n",
    "test(model, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do these parameters give better accuracy vs training from scratch?** Even with the topology and quantization fixed, achieving good accuracy on a given dataset requires [*hyperparameter tuning*](https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d) and potentially running training for a long time. The \"training from scratch\" example above is only intended as a quick example, whereas the pretrained parameters are obtained from a longer training run using the [determined.ai](https://determined.ai/) platform for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Surgery Before Export <a id=\"network_surgery\"></a>\n",
    "\n",
    "Sometimes, it's desirable to make some changes to our trained network prior to export (this is known in general as \"network surgery\"). This depends on the model and is not generally necessary, but in this case we want to make a couple of changes to get better results with FINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the model to CPU before surgery\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by padding the input. Our input vectors are 593-bit, which will make folding (parallelization) for the first layer a bit tricky since 593 is a prime number. So we'll pad the weight matrix of the first layer with seven 0-valued columns to work with an input size of 600 instead. When using the modified network we'll similarly provide inputs padded to 600 bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 593)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "modified_model = deepcopy(model)\n",
    "\n",
    "W_orig = modified_model[0].weight.data.detach().numpy()\n",
    "W_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 600)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# pad the second (593-sized) dimensions with 7 zeroes at the end\n",
    "W_new = np.pad(W_orig, [(0,0), (0,7)])\n",
    "W_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 600])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_model[0].weight.data = torch.from_numpy(W_new)\n",
    "modified_model[0].weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll modify the expected input/output ranges. In FINN, we prefer to work with bipolar {-1, +1} instead of binary {0, 1} values. To achieve this, we'll create a \"wrapper\" model that handles the pre/postprocessing as follows:\n",
    "\n",
    "* on the input side, we'll pre-process by (x + 1) / 2 in order to map incoming {-1, +1} inputs to {0, 1} ones which the trained network is used to. Since we're just multiplying/adding a scalar, these operations can be [*streamlined*](https://finn.readthedocs.io/en/latest/nw_prep.html#streamlining-transformations) by FINN and implemented with no extra cost.\n",
    "\n",
    "* on the output side, we'll add a binary quantizer which maps everthing below 0 to -1 and everything above 0 to +1. This is essentially the same behavior as the sigmoid we used earlier, except the outputs are bipolar instead of binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CybSecMLPForExport(\n",
       "  (pretrained): Sequential(\n",
       "    (0): QuantLinear(\n",
       "      in_features=593, out_features=64, bias=True\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "              (input_view_impl): Identity()\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): QuantLinear(\n",
       "      in_features=64, out_features=64, bias=True\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "              (input_view_impl): Identity()\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): QuantLinear(\n",
       "      in_features=64, out_features=64, bias=True\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Dropout(p=0.5, inplace=False)\n",
       "    (11): QuantReLU(\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (act_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "          (activation_impl): ReLU()\n",
       "          (tensor_quant): RescalingIntQuant(\n",
       "            (int_quant): IntQuant(\n",
       "              (float_to_int_impl): RoundSte()\n",
       "              (tensor_clamp_impl): TensorClamp()\n",
       "              (delay_wrapper): DelayWrapper(\n",
       "                (delay_impl): _NoDelay()\n",
       "              )\n",
       "              (input_view_impl): Identity()\n",
       "            )\n",
       "            (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
       "              (stats_input_view_shape_impl): OverTensorView()\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsPercentile()\n",
       "              )\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "              (restrict_scaling): _RestrictValue(\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (clamp_scaling): _ClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "              )\n",
       "              (restrict_inplace_preprocess): Identity()\n",
       "              (restrict_preprocess): Identity()\n",
       "            )\n",
       "            (int_scaling_impl): IntScaling()\n",
       "            (zero_point_impl): ZeroZeroPoint(\n",
       "              (zero_point): StatelessBuffer()\n",
       "            )\n",
       "            (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "              (bit_width): StatelessBuffer()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): QuantLinear(\n",
       "      in_features=64, out_features=1, bias=True\n",
       "      (input_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (output_quant): ActQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "      (weight_quant): WeightQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "        (tensor_quant): RescalingIntQuant(\n",
       "          (int_quant): IntQuant(\n",
       "            (float_to_int_impl): RoundSte()\n",
       "            (tensor_clamp_impl): TensorClampSte()\n",
       "            (delay_wrapper): DelayWrapper(\n",
       "              (delay_impl): _NoDelay()\n",
       "            )\n",
       "            (input_view_impl): Identity()\n",
       "          )\n",
       "          (scaling_impl): StatsFromParameterScaling(\n",
       "            (parameter_list_stats): _ParameterListStats(\n",
       "              (first_tracked_param): _ViewParameterWrapper(\n",
       "                (view_shape_impl): OverTensorView()\n",
       "              )\n",
       "              (stats): _Stats(\n",
       "                (stats_impl): AbsMax()\n",
       "              )\n",
       "            )\n",
       "            (stats_scaling_impl): _StatsScaling(\n",
       "              (affine_rescaling): Identity()\n",
       "              (restrict_clamp_scaling): _RestrictClampValue(\n",
       "                (clamp_min_ste): ScalarClampMinSte()\n",
       "                (restrict_value_impl): FloatRestrictValue()\n",
       "              )\n",
       "              (restrict_scaling_pre): Identity()\n",
       "              (restrict_scaling_impl): FloatRestrictValue()\n",
       "            )\n",
       "          )\n",
       "          (int_scaling_impl): IntScaling()\n",
       "          (zero_point_impl): ZeroZeroPoint(\n",
       "            (zero_point): StatelessBuffer()\n",
       "          )\n",
       "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (bias_quant): BiasQuantProxyFromInjector(\n",
       "        (_zero_hw_sentinel): StatelessBuffer()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qnt_output): QuantIdentity(\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (act_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
       "        (activation_impl): Identity()\n",
       "        (tensor_quant): ClampedBinaryQuant(\n",
       "          (scaling_impl): ConstScaling(\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_init_module): Identity()\n",
       "            (value): StatelessBuffer()\n",
       "          )\n",
       "          (bit_width): BitWidthConst(\n",
       "            (bit_width): StatelessBuffer()\n",
       "          )\n",
       "          (zero_point): StatelessBuffer()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "          (tensor_clamp_impl): TensorClamp()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from brevitas.nn import QuantIdentity\n",
    "\n",
    "\n",
    "class CybSecMLPForExport(nn.Module):\n",
    "    def __init__(self, my_pretrained_model):\n",
    "        super(CybSecMLPForExport, self).__init__()\n",
    "        self.pretrained = my_pretrained_model\n",
    "        self.qnt_output = QuantIdentity(\n",
    "            quant_type='binary', \n",
    "            scaling_impl_type='const',\n",
    "            bit_width=1, min_val=-1.0, max_val=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x contains bipolar {-1,1} elems\n",
    "        # shift from {-1,1} -> {0,1} since that is the\n",
    "        # input range for the trained network\n",
    "        x = (x + torch.tensor([1.0]).to(x.device)) / 2.0  \n",
    "        out_original = self.pretrained(x)\n",
    "        out_final = self.qnt_output(out_original)   # output as {-1,1}     \n",
    "        return out_final\n",
    "\n",
    "model_for_export = CybSecMLPForExport(modified_model)\n",
    "model_for_export.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_padded_bipolar(model, test_loader):    \n",
    "    # ensure model is in eval mode\n",
    "    model.eval() \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, target = data\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            # pad inputs to 600 elements\n",
    "            input_padded = torch.nn.functional.pad(inputs, (0,7,0,0))\n",
    "            # convert inputs to {-1,+1}\n",
    "            input_scaled = 2 * input_padded - 1\n",
    "            # run the model\n",
    "            output = model(input_scaled.float())\n",
    "            y_pred.extend(list(output.flatten().cpu().numpy()))\n",
    "            # make targets bipolar {-1,+1}\n",
    "            expected = 2 * target.float() - 1\n",
    "            expected = expected.cpu().numpy()\n",
    "            y_true.extend(list(expected.flatten()))\n",
    "        \n",
    "    return accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8065758149929554"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_padded_bipolar(model_for_export, test_quantized_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to QONNX and Conversion to FINN-ONNX <a id=\"export_qonnx\" ></a>\n",
    "\n",
    "\n",
    "[ONNX](https://onnx.ai/) is an open format built to represent machine learning models, and the FINN compiler expects an ONNX model as input. We'll now export our network into ONNX to be imported and used in FINN for the next notebooks. Note that the particular ONNX representation used for FINN differs from standard ONNX, you can read more about this [here](https://finn.readthedocs.io/en/latest/internals.html#intermediate-representation-finn-onnx).\n",
    "\n",
    "You can see below how we export a trained network in Brevitas into a FINN-compatible ONNX representation (QONNX). QONNX is the format we can export from Brevitas, to feed it into the FINN compiler, we will need to make a conversion to the FINN-ONNX format which is the intermediate representation the compiler works on. The conversion of the FINN-ONNX format is a FINN compiler transformation and to be able to apply it to our model, we will need to wrap it into [ModelWrapper](https://finn.readthedocs.io/en/latest/internals.html#modelwrapper). This is a wrapper around the ONNX model which provides several helper functions to make it easier to work with the model. Then we can call the conversion function to obtain the model in FINN-ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Installation of onnx and onnxoptimizer is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m model_for_export\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Export to ONNX\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mexport_qonnx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_for_export\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mready_model_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_t\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# # clean-up\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# qonnx_cleanup(ready_model_filename, out_file=ready_model_filename)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# print(\"Model saved to %s\" % ready_model_filename)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/brevitas/lib/python3.10/site-packages/brevitas/export/__init__.py:19\u001b[0m, in \u001b[0;36mexport_qonnx\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/brevitas/lib/python3.10/site-packages/brevitas/export/onnx/manager.py:198\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(cls, module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/brevitas/lib/python3.10/site-packages/brevitas/export/onnx/manager.py:116\u001b[0m, in \u001b[0;36mexport_onnx\u001b[0;34m(cls, module, args, export_path, input_shape, input_t, disable_warnings, **onnx_export_kwargs)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: Installation of onnx and onnxoptimizer is required."
     ]
    }
   ],
   "source": [
    "from brevitas.export import export_qonnx\n",
    "from qonnx.util.cleanup import cleanup as qonnx_cleanup\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.core.datatype import DataType\n",
    "#from finn.transformation.qonnx.convert_qonnx_to_finn import ConvertQONNXtoFINN\n",
    "\n",
    "ready_model_filename = \"plutoModel.onnx\"\n",
    "input_shape = (1, 600)\n",
    "\n",
    "model_for_export = model\n",
    "\n",
    "\n",
    "# create a QuantTensor instance to mark input as bipolar during export\n",
    "input_a = np.random.randint(0, 1, size=input_shape).astype(np.float32)\n",
    "input_a = 2 * input_a - 1\n",
    "scale = 1.0\n",
    "input_t = torch.from_numpy(input_a * scale)\n",
    "\n",
    "#Move to CPU before export\n",
    "model_for_export.cpu()\n",
    "\n",
    "# Export to ONNX\n",
    "export_qonnx(\n",
    "    model_for_export, export_path=ready_model_filename, input_t=input_t\n",
    ")\n",
    "\n",
    "# # clean-up\n",
    "# qonnx_cleanup(ready_model_filename, out_file=ready_model_filename)\n",
    "\n",
    "# # ModelWrapper\n",
    "# model = ModelWrapper(ready_model_filename)\n",
    "# # Setting the input datatype explicitly because it doesn't get derived from the export function\n",
    "# model.set_tensor_datatype(model.graph.input[0].name, DataType[\"BIPOLAR\"])\n",
    "# model = model.transform(ConvertQONNXtoFINN())\n",
    "# model.save(ready_model_filename)\n",
    "\n",
    "# print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the Exported ONNX in Netron\n",
    "\n",
    "Let's examine the exported ONNX model with [Netron](https://github.com/lutzroeder/netron), which is a visualizer for neural networks and allows interactive investigation of network properties. For example, you can click on the individual nodes and view the properties. Particular things of note:\n",
    "\n",
    "* The input tensor \"0\" is annotated with `quantization: finn_datatype: BIPOLAR`\n",
    "* The input preprocessing (x + 1) / 2 is exported as part of the network (initial `Add` and `Div` layers)\n",
    "* Brevitas `QuantLinear` layers are exported to ONNX as `MatMul`. We've exported the padded version; shape of the first MatMul node's weight parameter is 600x64\n",
    "* The weight parameters (second inputs) for MatMul nodes are annotated with `quantization: finn_datatype: INT2`\n",
    "* The quantized activations are exported as `MultiThreshold` nodes with `domain=qonnx.custom_op.general`\n",
    "* There's a final `MultiThreshold` node with threshold=0 to produce the final bipolar output (this is the `qnt_output` from `CybSecMLPForExport`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it! <a id=\"thats_it\" ></a>\n",
    "You created, trained and tested a quantized MLP that is ready to be loaded into FINN, congratulations! You can now proceed to the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
